name: Scrape & Update Links (Debug)

on:
  schedule:
    - cron: '0 */3 * * *'  # every 3 hours
  workflow_dispatch:        # manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repo
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Scrape page, extract data & update JSON file
        run: |
          #!/usr/bin/env bash
          set -euo pipefail

          URL="https://nirbytes.com/post/1000-proxies-for-school-chromebook-2025"
          HTML=$(curl -sL "$URL")

          echo "===== STARTING SCRAPE DEBUG ====="

          echo "üîç Dumping a few lines around the <h2 id=...>"
          # Wrap the pipeline in a subshell and ignore non-zero exit status due to broken pipe
          (printf "%s\n" "$HTML" | sed -n '/<h2 id="mcetoc_/,/<\/ul>/p' | head -20 2>/dev/null) || true

          echo ""
          echo "üîç Extracting TITLE from <h2 id=...>...</h2>"
          TITLE=$(printf "%s\n" "$HTML" \
            | grep '<h2 id="mcetoc_' \
            | head -1 \
            | sed -E 's/.*<h2 id="mcetoc_[^"]*">([^<]*)<\/h2>.*/\1/')
          echo "üìå TITLE: $TITLE"

          echo ""
          echo "üîç Extracting LINKS from following <ul> list"
          LINKS=$(printf "%s\n" "$HTML" \
            | sed -n '/<h2 id="mcetoc_/,/<\/ul>/p' \
            | grep '<li>' \
            | sed -E 's/.*<li>([^<]*)<\/li>.*/\1/' \
            | sed 's/&nbsp;//g')
          echo "üìé LINKS:"
          printf "%s\n" "$LINKS"

          # Convert the LINKS output (one per line) into a JSON array using jq.
          # The -R flag reads raw input, and -s collects all lines into a JSON array.
          LINKS_JSON=$(printf "%s\n" "$LINKS" | jq -R . | jq -s .)

          # Build JSON object containing the title and links.
          JSON_OUTPUT=$(jq -n --arg title "$TITLE" --argjson links "$LINKS_JSON" '{title: $title, links: $links}')

          # Write the JSON object to links.json in the repository root.
          echo "$JSON_OUTPUT" > links.json
          echo "üìù JSON file 'links.json' written with the scraped data."

      - name: Commit and push changes
        run: |
          git config --global user.email "action@github.com"
          git config --global user.name "GitHub Action"
          git add links.json
          if ! git diff-index --quiet HEAD; then
            git commit -m "Update links.json [skip ci]"
            git push
          else
            echo "No changes detected."
          fi
