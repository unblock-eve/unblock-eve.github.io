name: Scrape & Update Links

on:
  schedule:
    # every 3 hours
    - cron:  '0 */3 * * *'
  workflow_dispatch:   # manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repo
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Scrape page & build JSON
        run: |
          set -euo pipefail

          URL="https://example.com/your-page.html"  # ← your real URL
          HTML=$(curl -sL "$URL")

          # 1) extract the first <h2 id="mcetoc_…">…</h2> text
          TITLE=$(printf "%s\n" "$HTML" \
            | grep '<h2 id="mcetoc_' \
            | head -1 \
            | sed -E 's/.*<h2 id="mcetoc_[^"]*">([^<]*)<\/h2>.*/\1/')

          # 2) extract all <li>…</li> between that <h2> and its </ul>
          LINKS=$(printf "%s\n" "$HTML" \
            | sed -n '/<h2 id="mcetoc_/,/<\/ul>/p' \
            | grep '<li>' \
            | sed -E 's/.*<li>([^<]*)<\/li>.*/\1/' \
            | sed 's/&nbsp;//g')

          # 3) build JSON file
          mkdir -p data
          FILE=data/links.json
          [ -f "$FILE" ] || echo '{}' > "$FILE"

          # assemble array
          ARR=$(printf "%s\n" "$LINKS" | jq -R . | jq -s .)

          # merge under TITLE
          jq --arg title "$TITLE" --argjson arr "$ARR" \
             '.[$title] = $arr' "$FILE" > "$FILE.tmp" \
            && mv "$FILE.tmp" "$FILE"

          echo "✅ Updated '$FILE' with key: $TITLE"

      - name: Commit & push changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/links.json
          if git diff --cached --quiet; then
            echo "No changes to commit."
          else
            git commit -m "chore: update scraped links"
            git push
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
