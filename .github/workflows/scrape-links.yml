name: Scrape & Update Links (Debug)

on:
  schedule:
    - cron: '0 */3 * * *'  # every 3 hours
  workflow_dispatch:        # manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repo
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Scrape page, extract data & write JSON
        run: |
          #!/usr/bin/env bash
          set -euo pipefail

          URL="https://nirbytes.com/post/1000-proxies-for-school-chromebook-2025"
          HTML=$(curl -sL "$URL")

          echo "===== STARTING SCRAPE DEBUG ====="

          echo "ğŸ” Dumping a few lines around the <h2 id=...>"
          # Wrap the pipeline in a subshell and ignore non-zero exit status due to broken pipe
          (printf "%s\n" "$HTML" | sed -n '/<h2 id="mcetoc_/,/<\/ul>/p' | head -20 2>/dev/null) || true

          echo ""
          echo "ğŸ” Extracting TITLE from <h2 id=...>...</h2>"
          TITLE=$(printf "%s\n" "$HTML" \
            | grep '<h2 id="mcetoc_' \
            | head -1 \
            | sed -E 's/.*<h2 id="mcetoc_[^"]*">([^<]*)<\/h2>.*/\1/')
          echo "ğŸ“Œ TITLE: $TITLE"

          echo ""
          echo "ğŸ” Extracting LINKS from following <ul> list"
          LINKS=$(printf "%s\n" "$HTML" \
            | sed -n '/<h2 id="mcetoc_/,/<\/ul>/p' \
            | grep '<li>' \
            | sed -E 's/.*<li>([^<]*)<\/li>.*/\1/' \
            | sed 's/&nbsp;//g')
          echo "ğŸ“ LINKS:"
          printf "%s\n" "$LINKS"

          # Convert the LINKS output (one per line) into a JSON array using jq.
          # The -R flag reads raw input, and -s collects all lines into a JSON array.
          LINKS_JSON=$(printf "%s\n" "$LINKS" | jq -R . | jq -s .)

          # Build JSON object containing the title and links.
          JSON_OUTPUT=$(jq -n --arg title "$TITLE" --argjson links "$LINKS_JSON" '{title: $title, links: $links}')

          # Write the JSON object to links.json in the repository root.
          echo "$JSON_OUTPUT" > links.json
          echo "ğŸ“ JSON file 'links.json' written with the scraped data."

          echo ""
          echo "ğŸš« Skipping file write for additional updates (debug only)"
          exit 0
