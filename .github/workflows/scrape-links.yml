name: Scrape & Update Links (Debug)

on:
  schedule:
    - cron: '0 */3 * * *'  # every 3 hours
  workflow_dispatch:        # manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repo
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Scrape page & debug output
        run: |
          set -euo pipefail

          URL="https://nirbytes.com/post/1000-proxies-for-school-chromebook-2025"
          HTML=$(curl -sL "$URL")

          echo "===== STARTING SCRAPE DEBUG ====="

          echo "ğŸ” Dumping a few lines around the <h2 id=...>"
          # Suppress broken pipe warnings by redirecting stderr to /dev/null
          printf "%s\n" "$HTML" | sed -n '/<h2 id="mcetoc_/,/<\/ul>/p' | head -20 2>/dev/null

          echo ""
          echo "ğŸ” Extracting TITLE from <h2 id=...>...</h2>"
          TITLE=$(printf "%s\n" "$HTML" \
            | grep '<h2 id="mcetoc_' \
            | head -1 \
            | sed -E 's/.*<h2 id="mcetoc_[^"]*">([^<]*)<\/h2>.*/\1/')
          echo "ğŸ“Œ TITLE: $TITLE"

          echo ""
          echo "ğŸ” Extracting LINKS from following <ul> list"
          LINKS=$(printf "%s\n" "$HTML" \
            | sed -n '/<h2 id="mcetoc_/,/<\/ul>/p' \
            | grep '<li>' \
            | sed -E 's/.*<li>([^<]*)<\/li>.*/\1/' \
            | sed 's/&nbsp;//g')
          echo "ğŸ“ LINKS:"
          printf "%s\n" "$LINKS"

          echo ""
          echo "ğŸš« Skipping file write for now (debug only)"
          exit 0
