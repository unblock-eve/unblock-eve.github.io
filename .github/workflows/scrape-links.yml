name: Scrape & Update Links (Debug)

on:
  schedule:
    - cron: '0 */3 * * *'  # every 3 hours
  workflow_dispatch:        # manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repo
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Scrape page, extract sections & update JSON file
        run: |
          #!/usr/bin/env bash
          set -euo pipefail

          URL="https://nirbytes.com/post/1000-proxies-for-school-chromebook-2025"
          HTML=$(curl -sL "$URL")

          echo "===== STARTING SCRAPE DEBUG ====="
          echo "Scraping URL: $URL"

          # Use Perl to extract each section that starts with the relevant <h2> and its following <ul>.
          # The regex captures the title text and everything inside the <ul>...</ul> block.
          sections=$(echo "$HTML" | perl -0777 -ne 'while (/<h2 id="mcetoc_[^"]*">(.*?)<\/h2>\s*<ul>(.*?)<\/ul>/sg) {
              my $t = $1;
              my $u = $2;
              # Remove newlines from the ul block to make parsing easier.
              $u =~ s/\n/ /g;
              print "$t|||$u\n";
          }')

          if [[ -z "$sections" ]]; then
            echo "No sections found! Exiting."
            exit 1
          fi

          # Initialize an empty JSON array.
          json_array="[]"

          # Process each section found
          while IFS="|||" read -r title block; do
            # Clean the title by stripping any HTML tags, then trim whitespace.
            title=$(echo "$title" | sed -e 's/<[^>]*>//g' | xargs)
            echo "Found section with title: $title"

            # Extract individual links from the <li> items in the block.
            links=$(echo "$block" | grep -oP '<li>\K.*?(?=</li>)' | sed 's/&nbsp;//g')

            # Convert the list of links (one per line) into a JSON array.
            links_json=$(printf "%s\n" "$links" | jq -R . | jq -s .)

            # Build a JSON object for the current section.
            section_json=$(jq -n --arg title "$title" --argjson links "$links_json" '{title: $title, links: $links}')

            # Append this section object to our JSON array.
            json_array=$(echo "$json_array" | jq --argjson section "$section_json" '. + [$section]')
          done <<< "$sections"

          # Write the accumulated sections array to links.json in the repository root.
          echo "$json_array" > links.json
          echo "üìù JSON file 'links.json' written with the scraped data."

      - name: Commit and push changes
        run: |
          git config --global user.email "action@github.com"
          git config --global user.name "GitHub Action"
          git add links.json
          if ! git diff-index --quiet HEAD; then
            git commit -m "Update links.json [skip ci]"
            git push
          else
            echo "No changes detected; nothing to commit."
          fi
