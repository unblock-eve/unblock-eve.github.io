name: Scrape & Update Links

on:
  schedule:
    # every 3 hours
    - cron:  '0 */3 * * *'
  workflow_dispatch:   # manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repo
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Fetch & parse HTML in JS
        uses: actions/github-script@v7
        id: scraper
        with:
          script: |
            const fetch = require('node-fetch');
            const { JSDOM } = require('jsdom');
            const fs = require('fs');
            const path = 'data/links.json';

            // 1) Fetch page
            const url = 'https://example.com/your-page.html'; // ← your URL
            const res = await fetch(url);
            if (!res.ok) throw new Error(`HTTP ${res.status}`);
            const html = await res.text();

            // 2) Parse DOM
            const dom = new JSDOM(html);
            const doc = dom.window.document;
            const h2 = doc.querySelector('h2[id^="mcetoc_"]');
            if (!h2) throw new Error('No <h2 id="mcetoc_…"> found');
            const title = h2.textContent.trim();

            // 3) Grab next <ul> links
            let ul = h2.nextElementSibling;
            if (!ul || ul.tagName !== 'UL') throw new Error('No <ul> after <h2>');
            const links = Array.from(ul.querySelectorAll('li'))
                               .map(li => li.textContent.replace(/\u00a0/g, '').trim());

            // 4) Load existing JSON or start empty
            let data = {};
            if (fs.existsSync(path)) {
              data = JSON.parse(fs.readFileSync(path, 'utf8'));
            }
            // 5) Update and write
            data[title] = links;
            fs.mkdirSync('data', { recursive: true });
            fs.writeFileSync(path, JSON.stringify(data, null, 2));

            return { title, count: links.length };

      - name: Commit & push changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/links.json
          if git diff --cached --quiet; then
            echo "No changes to commit."
          else
            git commit -m "chore: update scraped links"
            git push
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
