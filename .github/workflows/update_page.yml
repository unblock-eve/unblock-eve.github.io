name: Update GitHub Pages with Scraped Links

on:
  schedule:
    - cron: "0 * * * *"  # Runs every hour
  workflow_dispatch:

jobs:
  update-pages:
    runs-on: ubuntu-latest

    steps:
      # Checkout repository to access files
      - name: Checkout repository
        uses: actions/checkout@v3

      # Set current timestamp in CST timezone
      - name: Set last updated timestamp in CST timezone
        id: timestamp
        run: |
          export TZ="America/Chicago"
          echo "LAST_UPDATED=$(date +"%B %d, %Y at %I:%M %p %Z")" >> $GITHUB_ENV

      # Fetch and scrape links from the target webpage
      - name: Scrape Links and Titles
        run: |
          # Define target URL
          URL="https://nirbytes.com/post/1000-proxies-for-school-chromebook-2024"

          # Use curl and grep to fetch and parse webpage content
          RESPONSE=$(curl -s "$URL")
          LINKS=$(echo "$RESPONSE" | grep -oP '(?<=href=")[^"]*')
          TITLES=$(echo "$RESPONSE" | grep -oP '(?<=<h2>).*?(?=</h2>)')

          # Create links.json file
          echo "{" > links.json

          # Process titles and their corresponding links
          while read -r TITLE && read -r LINK <&3; do
            echo "  \"$TITLE\": [" >> links.json
            echo "    \"$LINK\"" >> links.json
            echo "  ]," >> links.json
          done < <(echo "$TITLES") 3< <(echo "$LINKS")

          # Remove the last comma and close the JSON object
          sed -i '$ s/,$//' links.json
          echo "}" >> links.json

      # Commit and push changes to the repository
      - name: Commit and push changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add links.json
          git commit -m "Auto-update links.json with scraped links and titles"
          git push
