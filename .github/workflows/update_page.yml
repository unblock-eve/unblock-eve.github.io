      - name: Fetch and parse webpage into HTML
        run: |
          python - <<EOF
          import requests
          import re
          from requests.adapters import HTTPAdapter
          from requests.packages.urllib3.util.retry import Retry
          from bs4 import BeautifulSoup

          def extract_urls_from_text(text):
              url_pattern = r'https?://[^\s<>"\']+'
              return re.findall(url_pattern, text)

          url = "https://nirbytes.com/post/1000-proxies-for-school-chromebook-2024"
          headers = {
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
          }

          session = requests.Session()
          retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
          session.mount("https://", HTTPAdapter(max_retries=retries))
          response = session.get(url, headers=headers)
          soup = BeautifulSoup(response.content, 'html.parser')

          html_content = """
          <!DOCTYPE html>
          <html lang="en">
          <head>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1.0">
              <title>Extracted Proxy Links / RMA</title>
              <link rel="icon" type="image/x-icon" href="favicon.png">
              <style>
                  body {
                      font-family: 'Arial', sans-serif;
                      background-color: #121212;
                      color: #f5f5f5;
                      margin: 0;
                      padding: 0;
                      line-height: 1.6;
                  }

                  .container {
                      width: 80%;
                      max-width: 1200px;
                      margin: 30px auto;
                      padding: 20px;
                      background-color: #1f1f1f;
                      border-radius: 10px;
                      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
                  }

                  h1 {
                      text-align: center;
                      color: #66ccff;
                      font-size: 36px;
                      margin-bottom: 20px;
                  }

                  h2 {
                      color: #f5f5f5;
                      font-size: 28px;
                      margin-top: 20px;
                      border-bottom: 2px solid #444;
                      padding-bottom: 10px;
                  }

                  ul {
                      list-style-type: none;
                      padding-left: 0;
                      margin: 20px 0;
                  }

                  li {
                      margin: 10px 0;
                      font-size: 18px;
                      line-height: 1.4;
                  }

                  a {
                      color: #66ccff;
                      text-decoration: none;
                      transition: color 0.3s ease;
                  }

                  a:hover {
                      color: #ff9900;
                      text-decoration: underline;
                  }

                  p {
                      font-size: 18px;
                      line-height: 1.6;
                      color: #d0d0d0;
                  }

                  footer {
                      text-align: center;
                      font-size: 14px;
                      color: #888;
                      margin-top: 50px;
                  }
              </style>
          </head>
          <body>
              <div class="container">
                  <h1>Extracted Links and Titles</h1>
          """

          for h2 in soup.find_all('h2'):
              title = h2.get_text()
              html_content += f"<h2>{title}</h2><ul>"

              next_sibling = h2.find_next_sibling()
              while next_sibling and next_sibling.name in ['ul', 'p']:
                  if next_sibling.name == 'ul':
                      for li in next_sibling.find_all('li'):
                          if li.find('a'):
                              for a in li.find_all('a'):
                                  if a['href'].startswith('http'):
                                      html_content += f"<li><a href='{a['href']}' target='_blank'>{a['href']}</a></li>"
                          text_urls = extract_urls_from_text(li.get_text())
                          for url in text_urls:
                              if url not in [a['href'] for a in li.find_all('a')]:  
                                  html_content += f"<li><a href='{url}' target='_blank'>{url}</a></li>"
                  
                  elif next_sibling.name == 'p':
                      text_urls = extract_urls_from_text(next_sibling.get_text())
                      for url in text_urls:
                          html_content += f"<li><a href='{url}' target='_blank'>{url}</a></li>"
                  
                  next_sibling = next_sibling.find_next_sibling()

              html_content += "</ul>"

          html_content += """
              </div>
              <footer>
                  <p>Generated by GitHub Actions - Auto-update with extracted links</p>
              </footer>
          </body>
          </html>
          """

          with open('index.html', 'w', encoding='utf-8') as f:
              f.write(html_content)
          EOF
